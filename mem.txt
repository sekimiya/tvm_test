自分向け作業メモ
あとで綺麗にまとめたい

tvm公式チュートリアルより
tvmの各種python libにpathを通す
公式ドキュメントだとvtaにpathが通らないので、忘れずに追加する

# for tvm
export TVM_HOME=~/workspace/build/tvm/
export PYTHONPATH=$TVM_HOME/python:$TVM_HOME/topi/python:$TVM_HOME/nnvm/python:$TVM_HOME/vta/python:${PYTHONPATH}

llvmのbuild flagを立ててrebuildすればおｋ？
参考:https://github.com/dmlc/tvm/issues/966
todo:tvmのllvm build flagを探す
そもそもllvmのpathは通ってるような気がするけどなぜ...

参考:https://github.com/dmlc/tvm/issues/3080
https://discuss.tvm.ai/
同じようなerrorがtvmコミュでやれって誘導されていた
>>>Thanks for reporting this, usually we prefer to ask this kind of questions in https://discuss.tvm.ai/ please open a new thread there

llvmのver errorぽい、ver 4.0を入れると解決する？
https://discuss.tvm.ai/t/error-when-run-pynq-z1-test/1151
>>>Thanks. I have solved the problem. The problem is that I use the LLVM 7.0.0 version, but TVM now seems do not support 7.0.0 version. So I replace it with the LLVM 4.0.0 version. Then, the TVM VTA in pynq-Z1 work well.

sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ llc --version
LLVM (http://llvm.org/):
  LLVM version 3.8.0
  Optimized build.
  Built Jun 29 2018 (17:27:50).
  Default target: x86_64-unknown-linux-gnu
  Host CPU: broadwell

errorのerror、爆死
todo:依存関係の解決
sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ sudo apt-get install clang-4.0 lldb-4.0
Reading package lists... Done
Building dependency tree       
Reading state information... Done
clang-4.0 is already the newest version (1:4.0-1ubuntu1~16.04.2).
lldb-4.0 is already the newest version (1:4.0-1ubuntu1~16.04.2).
You might want to run 'apt-get -f install' to correct these:
The following packages have unmet dependencies:
 lldb-4.0 : Depends: python-lldb-4.0 but it is not going to be installed
E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).



sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ python3 build_code_for_target.py 
//IRの生成まではうまく行く
tvm.lower(s, [A, B, C], simple_mode=True)
produce C {
  for (i, 0, m) {
    for (j, 0, n) {
      C[((i*n) + j)] = (A[((i*n) + j)]*B[((i*n) + j)])
    }
  }
}

Traceback (most recent call last):
//targetを指定してのcode genで失敗する
  File "build_code_for_target.py", line 20, in <module>
    m = tvm.build(f, target="llvm")

  File "/home/sekiy/workspace/build/tvm/python/tvm/build_module.py", line 627, in build
    mhost = codegen.build_module(fhost_all, str(target_host))

  File "/home/sekiy/workspace/build/tvm/python/tvm/codegen.py", line 36, in build_module
    return _Build(lowered_func, target)

  File "/home/sekiy/workspace/build/tvm/python/tvm/_ffi/_ctypes/function.py", line 210, in __call__
    raise get_last_ffi_error()

tvm._ffi.base.TVMError: Traceback (most recent call last):
  [bt] (2) /home/sekiy/workspace/build/tvm/build/libtvm.so(TVMFuncCall+0x61) [0x7f291e2e5c51]
  [bt] (1) /home/sekiy/workspace/build/tvm/build/libtvm.so(+0x4052fe) [0x7f291dbb12fe]
  [bt] (0) /home/sekiy/workspace/build/tvm/build/libtvm.so(tvm::codegen::Build(tvm::Array<tvm::LoweredFunc, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0xcb4) [0x7f291dcbdbd4]
  File "/home/sekiy/workspace/build/tvm/src/codegen/codegen.cc", line 46
TVMError: Check failed: bf != nullptr: Target llvm is not enabled

llvmのverを6.0.1にしてtvmをrebuildしたらtarget=llvmでcode gen成功
tvmの利用するllvmは4.0以上にしておく、しかし10は非対応で同じくerror
https://github.com/llvm-mirror/llvm/tree/release_60

config.cmakeの一部
# Whether build with LLVM support
# Requires LLVM version >= 4.0
#
# Possible values:
# - ON: enable llvm with cmake's find search
# - OFF: disable llvm
# - /path/to/llvm-config: enable specific LLVM when multiple llvm-dev is available.
set(USE_LLVM /usr/local/bin/llvm-config)

code生成成功（表示はIR）
sekiy@gpu:~/workspace/tvm_test$ python3 build_code_for_target.py 
tvm.lower(s, [A, B, C], simple_mode=True)
produce C {
  for (i, 0, m) {
    for (j, 0, n) {
      C[((i*n) + j)] = (A[((i*n) + j)]*B[((i*n) + j)])
    }
  }
}

hardware向けのsucheduling の実装を追いかける

vta_get_started.pyのschedulingの一部
s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)

pragma以下がhard向けの実装？

scheduling実装はpython/tvm/scheduling.pyにまとまっている

def pragma():のコメントを一部引用
loopに対するアノテーション？
DMA転送をしているっぽいので、soft的に最適化したloopに対するhard特有の操作はpragma()で行うようなノリ？
        """Annotate the iteration with pragma

        This will translate to a pragma_scope surrounding
        the corresponding loop generated.
        Useful to support experimental features and extensions.

    def pragma(self, var, pragma_type, pragma_value=None):
        if isinstance(pragma_value, string_types):
            pragma_value = convert(pragma_value)
	    //_StagePragma()の実態はなんなのか...(Python側に実装がない
        _api_internal._StagePragma(self, var, pragma_type, pragma_value)

_api_internal.pyのコメント抜粋
_で始めるpythonのAPIの実態はC++側にある

"""Namespace of internal API

The functions in this namespace are automatically exported from C++ side via PackedFunc
that is registered by "TVM_REGISTER_*" macro. This way makes calling Python functions from C++
side very easily.

Each string starts with "_" in the "TVM_REGISTER_*" macro is an internal API. You can find
all the functions in "api_lang.cc", "api_base.cc", "api_arith.cc" and "api_ir.cc" under "src/api".
"""

todo:src/tvm/api/*.ccを調査

src/tvm/api/*.ccを筆頭にしたC++側のAPI郡は、llvmのAPIを利用して、IRを走査してpragma()の第三引数のenv.dma_copyなどで示されるloop構造の変更挿入用のpragmaを打ち込む？
pragmaはPython側の実装でユーザ定義のコードと置き換えられる？
todo : Python側のコード生成ロジックを調査

独自アーキテクチャ向けのオリジナルのスケジュールを実装するには？


def _inject_copy(src, dst, pad_before, pad_after, pad_value):

//vta_age_started.pyのdma_copyの実装を参考に調査する
s[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)
→ 
//このように展開される
produce B_buf {
  VTALoadBuffer2D(tvm_thread_context(VTATLSCommandHandle()), B, 0, 64, 1, 64, 0, 0, 0, 0, 64, 3)
}

env.dma_copyについて
environment.py, class Environment(object), C++のllvmをPythonから呼び出し、木構造を舐め、指定場所に展開用のpragma()を利用して独自定義のpragma挿入する
    @property
    def dma_copy(self):
        """DMA copy pragma"""
        return ("dma_copy"
                if not self.mock_mode
                else "skip_dma_copy")
挿入したpragmaは
vta.lower(s, [A, B, C], simple_mode=True)
によって展開される
todo : lower()の調査
固有のpragmaを展開する部分があったので、独自スケジューリング追加のためには実装が必要？

ir_pass.py, 
def _inject_copy(src, dst, pad_before, pad_after, pad_value):
-------中略
//以下のようにVTALoadBuffer2Dの定義がある
//これlowerとどうやって繋がるんだろう...
            irb.emit(tvm.call_extern(
                "int32", "VTALoadBuffer2D",
                env.dev.command_handle,
                src.data, offset, x_size, y_size, x_stride,
                x_pad_before, y_pad_before,
                x_pad_after, y_pad_after,
                dst.access_ptr("r", "int32"), mem_type))
            return irb.get()

実際に生成されたコードからcallされるVTALoadBuffer2Dの実態は、vta/src/runtime.cc, void VTALoadBuffer2D()にある

独自アーキテクチャ向けの専用スケジューリング実装のためのまとめ
・pragmaの定義
IR段階ではまず、単にpragmaをIRの木構造に埋め込む必要がある
この段階では、実際のコードではなく、単なるpragmaとして扱われる
pragmaはvta/python/vta/environment.pyにdef dma_copy(self)のように定義する
//environment.py, pragmaを定義する関数のサンプル
    @property
    def dma_copy(self):
        """DMA copy pragma"""
        return ("dma_copy"
                if not self.mock_mode
                else "skip_dma_copy")

・pragmaを挿入したIRからのcode gen
pragmaを挿入したIRはvta.lower(s, [A, B, C], simple_mode=True)のようにvta.lower()メソッドでpragmaが展開され、実際に定義されたC++の関数をcallするコードにloweringされる
例えば以下のように展開される

全コードはvta_get_started.pyを参照
//対応するtvmのコード
A_buf = tvm.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: A(*i), "A_buf")
//単なるbufferのcopy
produce A_buf {
  for (i1, 0, 64) {
    for (i3, 0, 16) {
      A_buf[((i1*16) + i3)] = A[((i1*16) + i3)]
    }
  }
}

//loopのscope?あとで調べる
s[A_buf].set_scope(env.acc_scope)
//DMA転送用のpragmaを挿入
s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)

//対応するloweringされたコード
// attr [A_buf] storage_scope = "local.acc_buffer"
// attr [iter_var(vta, , vta)] coproc_scope = 2
produce A_buf {
  //定義されたC++のfunctionが呼ばれる
  VTALoadBuffer2D(tvm_thread_context(VTATLSCommandHandle()), A, 0, 64, 1, 64, 0, 0, 0, 0, 0, 3)
}

todo : pragmaを入れた時のcodeのloweingの実装詳細
IRからIRにllvmのpathで変換している？

tvmとvtaの界面がコードレベルでどうなってるのかよくわからないのでもう少し読む
todo : lower()のあとのbuild()も読む

気になるメソッド
def _build_for_device(flist, target, target_host):
def build(inputs,
          args=None,
          target=None,
          target_host=None,
          name="default_function",
          binds=None):

ir_pass.pyにDMA転送など独自pragmaを刺す？展開する？classが並んでいる
ファイル名的にIR2IRな気がする
inputにハードウェア固有のスケジューリングが入ってないIRをpragma()でとって、ハードウェア固有のコードの入ったIRに変換する？

vta_conv2d.py
autotvm向けのスケジューリング自動調整用のことがいろいろ書いてある？
autotvmをつかって自動チューニングを行うにはここも実装の必要が有る？

_で始まるのはC++に実装の実態がある

ir_pass.pyでpragmaを実際の定義コードに変換する
//ir_pass.pyのコメントを一部抜粋
def inject_dma_intrin(stmt_in):
    """Pass to inject DMA copy intrinsics.

    Parameters
    ----------
    stmt_in : Stmt
        Input statement

    Returns
    -------
    stmt_out : Stmt
        Transformed statement
    """

inject_dma_intrin()はbuild_module.pyから呼ばれている
//build_module.pyの一部
def build_config(debug_flag=0, **kwargs):
    """Build a build config for VTA.

    Parameters
    ----------
    debug_flag : int
        The dbeug flag to be passed.

    kwargs : dict
        Additional configurations.

    Returns
    -------
    build_config: BuildConfig
        The build config that can be used in TVM.

    Example
    --------
    .. code-block:: python

      # build a vta module.
      with vta.build_config():
          vta_module = tvm.build(s, ...)
    """
    env = get_env()
    def add_debug(stmt):
        debug = tvm.call_extern(
            "int32", "VTASetDebugMode",
            env.dev.command_handle,
            debug_flag)

        return tvm.make.stmt_seq(debug, stmt)
        //ここでcall
    pass_list = [(1, ir_pass.inject_dma_intrin),
                 (1, ir_pass.inject_skip_copy),
                 (1, ir_pass.annotate_alu_coproc_scope),
                 (1, lambda x: tvm.ir_pass.LiftAttrScope(x, "coproc_uop_scope", True)),
                 (1, lift_coproc_scope),
                 (1, ir_pass.inject_coproc_sync),
                 (1, early_rewrite)]
    if debug_flag:
        pass_list.append((1, add_debug))
    pass_list.append((2, ir_pass.inject_alu_intrin))
    pass_list.append((3, ir_pass.fold_uop_loop))
    pass_list.append((3, ir_pass.cpu_access_rewrite))
    return tvm.build_config(add_lower_pass=pass_list, **kwargs)

    大枠としては、pragma挿入（pragma()）、pragmaをヒントに定義コードに展開(lower()→build_config()→inject_dma_intrin())、展開先のハードウェア用のAPIはC++にて定義され、別途、生成されたC++コードから呼び出される
    新規スケジュールの実装には、
   
    C++側
    ・ハードのdraiver APIの繋ぎこみ
    ・Pythonから呼び出される粒度でのAPIの定義

    Python側
    ・専用命令用のpragmaの定義
    ・ソフトウェア最適化後のIRをC++で定義した処理単位でのAPIを呼び出すIRに変換するためのpass定義
    
    結局
    デバイス側 → pragma()で展開されるAPIの粒度まで抽象化して実装すればなんでもあり。IRレベルで挿入したい処理の粒度を意識して必要な処理を実装する
    きっちりしたAPIが定義されているデバイスなら、SDKを一緒にbuildして、そのAPIを呼び出す程度でOK？
    コンパイラ側 → スケジュールで生成されたIRをpragma()でハードウェアのAPIに展開するIR Passの実装が肝、ir_pass.pyに実装がまとまっているので近々はここを見る
    
    todo : pragma()でのIR2IRのpass実装調査（ir_pass.py）

ir_passの実装でやること
・ハードウェア向けのpragmaが刺さったIRを舐める
・pragmaとスケジューリングを展開したIRをC++で実装されたハードウェア向けのAPIへと置き換える
  どうやって、IRを舐めて、C+＋APIへと置き換えているのか？ → ir_pass.pyの inject_dma_intrin()メソッドなどを調査

IRの変換
low_level_custum_pass.pyがいろいろ役立ちそう
  //コメント抜粋
  """
  Writing a Customized Pass
  =========================
  **Author**: `Jian Weng <https://were.github.io>`_

  TVM is a framework that abstracts away the heterogenity of machine learning accelerators.
  Sometimes users may want customize some analysis and IR transformations
  to adapt TVM to their own specialized hardware. This tutorial helps users write
  a customized pass in TVM.

  Prerequisites
  -------------

  Before reading this tutorial, we assume readers have already known these topics well:

  - Writing an algorithm in TVM and schedule it. Otherwise, see example tutorials like
    :ref:`opt-gemm`.
  - The basic structure of HalideIR. Otherwise, see ``HalideIR/src/ir/IR.h`` to learn what
    attributes of IR nodes are defined.
  - Visitor design pattern. Otherwise, check the
    `Python AST module <https://docs.python.org/3/library/ast.html>`_ to see how an AST
    visitor is implemented.
  - How a HalideIR/Schedule is lowered to either a LoweredFunc class or a LLVM module. Otherwise,
    take a look at ``python/tvm/build_module.py`` to get some basics.

  """


