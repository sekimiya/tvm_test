tvm公式チュートリアルより
tvmの各種python libにpathを通す
公式ドキュメントだとvtaにpathが通らないので、忘れずに追加する

# for tvm
export TVM_HOME=~/workspace/build/tvm/
export PYTHONPATH=$TVM_HOME/python:$TVM_HOME/topi/python:$TVM_HOME/nnvm/python:$TVM_HOME/vta/python:${PYTHONPATH}

llvmのbuild flagを立ててrebuildすればおｋ？
参考:https://github.com/dmlc/tvm/issues/966
todo:tvmのllvm build flagを探す
そもそもllvmのpathは通ってるような気がするけどなぜ...

参考:https://github.com/dmlc/tvm/issues/3080
https://discuss.tvm.ai/
同じようなerrorがtvmコミュでやれって誘導されていた
>>>Thanks for reporting this, usually we prefer to ask this kind of questions in https://discuss.tvm.ai/ please open a new thread there

llvmのver errorぽい、ver 4.0を入れると解決する？
https://discuss.tvm.ai/t/error-when-run-pynq-z1-test/1151
>>>Thanks. I have solved the problem. The problem is that I use the LLVM 7.0.0 version, but TVM now seems do not support 7.0.0 version. So I replace it with the LLVM 4.0.0 version. Then, the TVM VTA in pynq-Z1 work well.

sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ llc --version
LLVM (http://llvm.org/):
  LLVM version 3.8.0
  Optimized build.
  Built Jun 29 2018 (17:27:50).
  Default target: x86_64-unknown-linux-gnu
  Host CPU: broadwell

errorのerror、爆死
todo:依存関係の解決
sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ sudo apt-get install clang-4.0 lldb-4.0
Reading package lists... Done
Building dependency tree       
Reading state information... Done
clang-4.0 is already the newest version (1:4.0-1ubuntu1~16.04.2).
lldb-4.0 is already the newest version (1:4.0-1ubuntu1~16.04.2).
You might want to run 'apt-get -f install' to correct these:
The following packages have unmet dependencies:
 lldb-4.0 : Depends: python-lldb-4.0 but it is not going to be installed
E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).



sekiy@gpu:~/workspace/build/tvm/tutorials/seki_tvm_test$ python3 build_code_for_target.py 
//IRの生成まではうまく行く
tvm.lower(s, [A, B, C], simple_mode=True)
produce C {
  for (i, 0, m) {
    for (j, 0, n) {
      C[((i*n) + j)] = (A[((i*n) + j)]*B[((i*n) + j)])
    }
  }
}

Traceback (most recent call last):
//targetを指定してのcode genで失敗する
  File "build_code_for_target.py", line 20, in <module>
    m = tvm.build(f, target="llvm")

  File "/home/sekiy/workspace/build/tvm/python/tvm/build_module.py", line 627, in build
    mhost = codegen.build_module(fhost_all, str(target_host))

  File "/home/sekiy/workspace/build/tvm/python/tvm/codegen.py", line 36, in build_module
    return _Build(lowered_func, target)

  File "/home/sekiy/workspace/build/tvm/python/tvm/_ffi/_ctypes/function.py", line 210, in __call__
    raise get_last_ffi_error()

tvm._ffi.base.TVMError: Traceback (most recent call last):
  [bt] (2) /home/sekiy/workspace/build/tvm/build/libtvm.so(TVMFuncCall+0x61) [0x7f291e2e5c51]
  [bt] (1) /home/sekiy/workspace/build/tvm/build/libtvm.so(+0x4052fe) [0x7f291dbb12fe]
  [bt] (0) /home/sekiy/workspace/build/tvm/build/libtvm.so(tvm::codegen::Build(tvm::Array<tvm::LoweredFunc, void> const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)+0xcb4) [0x7f291dcbdbd4]
  File "/home/sekiy/workspace/build/tvm/src/codegen/codegen.cc", line 46
TVMError: Check failed: bf != nullptr: Target llvm is not enabled

llvmのverを6.0.1にしてtvmをrebuildしたらtarget=llvmでcode gen成功
tvmの利用するllvmは4.0以上にしておく、しかし10は非対応で同じくerror
https://github.com/llvm-mirror/llvm/tree/release_60

config.cmakeの一部
# Whether build with LLVM support
# Requires LLVM version >= 4.0
#
# Possible values:
# - ON: enable llvm with cmake's find search
# - OFF: disable llvm
# - /path/to/llvm-config: enable specific LLVM when multiple llvm-dev is available.
set(USE_LLVM /usr/local/bin/llvm-config)

code生成成功（表示はIR）
sekiy@gpu:~/workspace/tvm_test$ python3 build_code_for_target.py 
tvm.lower(s, [A, B, C], simple_mode=True)
produce C {
  for (i, 0, m) {
    for (j, 0, n) {
      C[((i*n) + j)] = (A[((i*n) + j)]*B[((i*n) + j)])
    }
  }
}

hardware向けのsucheduling の実装を追いかける

vta_get_started.pyのschedulingの一部
s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)

pragma以下がhard向けの実装？

scheduling実装はpython/tvm/scheduling.pyにまとまっている

def pragma():のコメントを一部引用
loopに対するアノテーション？
DMA転送をしているっぽいので、soft的に最適化したloopに対するhard特有の操作はpragma()で行うようなノリ？
        """Annotate the iteration with pragma

        This will translate to a pragma_scope surrounding
        the corresponding loop generated.
        Useful to support experimental features and extensions.

    def pragma(self, var, pragma_type, pragma_value=None):
        if isinstance(pragma_value, string_types):
            pragma_value = convert(pragma_value)
	    //_StagePragma()の実態はなんなのか...(Python側に実装がない
        _api_internal._StagePragma(self, var, pragma_type, pragma_value)

_api_internal.pyのコメント抜粋
_で始めるpythonのAPIの実態はC++側にある

"""Namespace of internal API

The functions in this namespace are automatically exported from C++ side via PackedFunc
that is registered by "TVM_REGISTER_*" macro. This way makes calling Python functions from C++
side very easily.

Each string starts with "_" in the "TVM_REGISTER_*" macro is an internal API. You can find
all the functions in "api_lang.cc", "api_base.cc", "api_arith.cc" and "api_ir.cc" under "src/api".
"""

todo:src/tvm/api/*.ccを調査

src/tvm/api/*.ccを筆頭にしたC++側のAPI郡は、llvmのAPIを利用して、IRを走査してpragma()の第三引数のenv.dma_copyなどで示されるloop構造の変更挿入用のpragmaを打ち込む？
pragmaはPython側の実装でユーザ定義のコードと置き換えられる？
todo : Python側のコード生成ロジックを調査

独自アーキテクチャ向けのオリジナルのスケジュールを実装するには？


def _inject_copy(src, dst, pad_before, pad_after, pad_value):

//vta_age_started.pyのdma_copyの実装を参考に調査する
s[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)
→ 
//このように展開される
produce B_buf {
  VTALoadBuffer2D(tvm_thread_context(VTATLSCommandHandle()), B, 0, 64, 1, 64, 0, 0, 0, 0, 64, 3)
}

env.dma_copyについて
environment.py, class Environment(object), C++のllvmをPythonから呼び出し、木構造を舐め、指定場所に展開用のpragma()を利用して独自定義のpragma挿入する
    @property
    def dma_copy(self):
        """DMA copy pragma"""
        return ("dma_copy"
                if not self.mock_mode
                else "skip_dma_copy")
挿入したpragmaは
vta.lower(s, [A, B, C], simple_mode=True)
によって展開される
todo : lower()の調査

ir_pass.py, 
def _inject_copy(src, dst, pad_before, pad_after, pad_value):
-------中略
//以下のようにVTALoadBuffer2Dの定義がある
//これlowerとどうやって繋がるんだろう...
            irb.emit(tvm.call_extern(
                "int32", "VTALoadBuffer2D",
                env.dev.command_handle,
                src.data, offset, x_size, y_size, x_stride,
                x_pad_before, y_pad_before,
                x_pad_after, y_pad_after,
                dst.access_ptr("r", "int32"), mem_type))
            return irb.get()

実際に生成されたコードからcallされるVTALoadBuffer2Dの実態は、vta/src/runtime.cc, void VTALoadBuffer2D()にある

独自アーキテクチャ向けの専用スケジューリング実装のためのまとめ
・pragmaの定義
IR段階ではまず、単にpragmaをIRの木構造に埋め込む必要がある
この段階では、実際のコードではなく、単なるpragmaとして扱われる
pragmaはvta/python/vta/environment.pyにdef dma_copy(self)のように定義する
//pragmaを定義する関数のサンプル
    @property
    def dma_copy(self):
        """DMA copy pragma"""
        return ("dma_copy"
                if not self.mock_mode
                else "skip_dma_copy")

・pragmaを挿入したIRからのcode gen
pragmaを挿入したIRはvta.lower(s, [A, B, C], simple_mode=True)のようにvta.lower()メソッドでpragmaが展開され、実際に定義されたC++の関数をcallするコードにloweringされる
例えば以下のように展開される

全コードはvta_get_started.pyを参照
//対応するtvmのコード
A_buf = tvm.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: A(*i), "A_buf")
//単なるbufferのcopy
produce A_buf {
  for (i1, 0, 64) {
    for (i3, 0, 16) {
      A_buf[((i1*16) + i3)] = A[((i1*16) + i3)]
    }
  }
}

//loopのscope?あとで調べる
s[A_buf].set_scope(env.acc_scope)
//DMA転送用のpragmaを挿入
s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)

//対応するloweringされたコード
// attr [A_buf] storage_scope = "local.acc_buffer"
// attr [iter_var(vta, , vta)] coproc_scope = 2
produce A_buf {
  //定義されたC++のfunctionが呼ばれる
  VTALoadBuffer2D(tvm_thread_context(VTATLSCommandHandle()), A, 0, 64, 1, 64, 0, 0, 0, 0, 0, 3)
}

